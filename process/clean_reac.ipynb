{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore public HUD REAC data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Jack Vandeleuv\n",
    "\n",
    "Data files:\n",
    "* [Public Housing Physical Inspection Scores (2016-2021)](https://www.huduser.gov/portal/datasets/pis.html#2021_data-collapse)\n",
    "* [Multifamily Physical Inspection Scores (2016-2021)](https://www.huduser.gov/portal/datasets/pis.html#2021_data-collapse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will explore the publicly released scores from the U.S. Department of Housing and Urban Development's Real Estate Assessment Center (HUD REAC). These scores are derived from physical inspections on HUD properties."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to aggregate REAC data on a city-by-city basis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the data files in the data directory and load into pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY_IN = '../data/raw/reac/'\n",
    "MULTIFAMILY_FILES = [\n",
    "    'multifamily_physical_inspection_scores_0321.xlsx',\n",
    "    'multifamily_physical_inspection_scores_0620.xlsx',\n",
    "    'multifamily-physical-inspection-scores-2016.xlsx',\n",
    "    'multifamily-physical-inspection-scores-2018.xlsx',\n",
    "    'multifamily-physical-inspection-scores-2019.xlsx'\n",
    "]\n",
    "\n",
    "PUBLIC_HOUSING_FILES = [\n",
    "    'public_housing_physical_inspection_scores_0321.xlsx',\n",
    "    'public_housing_physical_inspection_scores_0620.xlsx',\n",
    "    'public-housing-physical-inspection-scores-2016.xlsx',\n",
    "    'public-housing-physical-inspection-scores-2018.xlsx',\n",
    "    'public-housing-physical-inspection-scores-2019.xlsx'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The zipcode columns are inconsistently named, so we'll rename ZIPCODE to ZIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_dfs = [\n",
    "    pd.read_excel(WORKING_DIRECTORY_IN + file)\n",
    "    .rename(columns={\n",
    "        'ZIPCODE': 'ZIP',\n",
    "        'DEVELOPMENT_ID': 'PLACE_ID',\n",
    "        'DEVELOPMENT_NAME': 'PLACE_NAME',\n",
    "        'ADDRESS ': 'ADDRESS'\n",
    "        })\n",
    "    .assign(INSPECTION_TYPE='PUBLIC')\n",
    "    for file in PUBLIC_HOUSING_FILES\n",
    "]\n",
    "\n",
    "multi_dfs = [\n",
    "    pd.read_excel(WORKING_DIRECTORY_IN + file)\n",
    "    .rename(columns={\n",
    "        'ZIPCODE': 'ZIP',\n",
    "        'PROPERTY_ID': 'PLACE_ID',\n",
    "        'PROPERTY_NAME': 'PLACE_NAME',\n",
    "        })\n",
    "    .assign(INSPECTION_TYPE='MULTIFAMILY')\n",
    "    for file in MULTIFAMILY_FILES\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove PHA_CODE and PHA_NAME from the public housing dfs so the columns match with the multifamily dfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP = ['PHA_CODE', 'PHA_NAME']\n",
    "for i, df in enumerate(public_dfs):\n",
    "    public_dfs[i] = df.drop(columns=[\n",
    "        col for col in COLUMNS_TO_DROP \n",
    "        if col in df.columns])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll combine all the dataframes together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_dfs.extend(public_dfs)\n",
    "df = pd.concat(multi_dfs, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicate inspection (any records that share an inspection id)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset='INSPECTION_ID')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the null percentage in each column.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FIPS_STATE_CODE     81.44\n",
       "STATE_NAME          18.56\n",
       "CBSA_NAME            9.33\n",
       "ADDRESS              1.92\n",
       "CBSA_CODE            0.04\n",
       "COUNTY_NAME          0.04\n",
       "COUNTY_CODE          0.04\n",
       "LATITUDE             0.04\n",
       "LONGITUDE            0.04\n",
       "ZIP                  0.03\n",
       "LOCATION_QUALITY     0.03\n",
       "PLACE_NAME           0.00\n",
       "INSPECTION_SCORE     0.00\n",
       "INSPECTION_TYPE      0.00\n",
       "INSPECTION_DATE      0.00\n",
       "STATE_CODE           0.00\n",
       "PLACE_ID             0.00\n",
       "CITY                 0.00\n",
       "INSPECTION_ID        0.00\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.isnull().mean() * 100).round(2).sort_values(ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "City, state code, and inspection score don't have any null values, which is good. For now, we'll leave the other columns with nulls."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have two different methods of specifying state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATE_NAME</th>\n",
       "      <th>STATE_CODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20332</th>\n",
       "      <td>AL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5737</th>\n",
       "      <td>KY</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>NaN</td>\n",
       "      <td>IL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22709</th>\n",
       "      <td>TX</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18484</th>\n",
       "      <td>NaN</td>\n",
       "      <td>MN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      STATE_NAME STATE_CODE\n",
       "20332         AL          1\n",
       "5737          KY         21\n",
       "4047         NaN         IL\n",
       "22709         TX         48\n",
       "18484        NaN         MN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(n=5, random_state=6).loc[:, ['STATE_NAME', 'STATE_CODE']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have two different state code formats. We'll combine these into a single column with only the two-letter state codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(row):\n",
    "    if pd.isnull(row['STATE_NAME']):\n",
    "        return row['STATE_CODE']\n",
    "    else:\n",
    "        return row['STATE_NAME']\n",
    "\n",
    "df['STATE'] = df.apply(combine_columns, axis=1)\n",
    "df = df.drop(columns=['STATE_NAME', 'STATE_CODE'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the city names to be upper-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CITY = df.CITY.str.upper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the datetimes to a standard format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INSPECTION_DATE'] = pd.to_datetime(df['INSPECTION_DATE'], \n",
    "                                       infer_datetime_format=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at this distribution of inspections by date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "INSPECTION_TYPE  INSPECTION_DATE\n",
       "MULTIFAMILY      2013                2949\n",
       "                 2014                8542\n",
       "                 2015               10762\n",
       "                 2016                9104\n",
       "                 2017                7471\n",
       "                 2018               11856\n",
       "                 2019               11286\n",
       "                 2020                1835\n",
       "                 2021                   2\n",
       "PUBLIC           2005                   1\n",
       "                 2012                   5\n",
       "                 2013                 336\n",
       "                 2014                1739\n",
       "                 2015                3272\n",
       "                 2016                2424\n",
       "                 2017                3302\n",
       "                 2018                2438\n",
       "                 2019                2566\n",
       "                 2020                 879\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(by=['INSPECTION_TYPE', df.INSPECTION_DATE.dt.year]).size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of inspections in the dataset occurred between 2013 and 2020, with a drop-off when the 2019 pandemic started. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the pre-2013 data and the post-2020 data, which is only a few inspections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df.INSPECTION_DATE.dt.year >= 2013) &\n",
    "    (df.INSPECTION_DATE.dt.year <= 2020)\n",
    "]   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the minimum and maximum length of city names as a simple validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest names: ['HOT SPRINGS NATIONAL PARK']\n",
      "Shortest names ['ADA' 'IVA' 'LEE' 'ONA' 'DIX' 'AVA' 'RYE' 'OPP' 'OLA' 'ELY' 'BAY' 'RIO'\n",
      " 'ROY' 'ORD' 'YOE' 'VAN' 'INA']\n"
     ]
    }
   ],
   "source": [
    "print('Longest names:', df.loc[df.CITY.str.len() == df.CITY.str.len().max()].CITY.unique())\n",
    "print('Shortest names', df.loc[df.CITY.str.len() == df.CITY.str.len().min()].CITY.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the range of inspection scores for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0 \n",
      "Max: 100\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Min:', df.INSPECTION_SCORE.min(), \n",
    "    '\\nMax:', df.INSPECTION_SCORE.max()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 55 unique states represented, with some like GU representing territories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique state names: 55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['IN', 'ID', 'CA', 'NY', 'MO', 'UT', 'TX', 'MA', 'NJ', 'NM', 'TN',\n",
       "       'IL', 'CT', 'PA', 'NC', 'OH', 'AZ', 'IA', 'MI', 'RI', 'KY', 'MD',\n",
       "       'GA', 'OR', 'SD', 'WY', 'FL', 'MT', 'ME', 'CO', 'MN', 'WA', 'VA',\n",
       "       'WI', 'LA', 'AR', 'OK', 'KS', 'MS', 'NE', 'SC', 'PR', 'WV', 'NH',\n",
       "       'NV', 'VI', 'DC', 'AL', 'VT', 'AK', 'ND', 'HI', 'DE', 'MP', 'GU'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Unique state names:', len(df.STATE.unique()))\n",
    "df.STATE.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export our cleaned data from 2013-2019. (Reserve the 2020 data as a validation set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_13_19 = df[df.INSPECTION_DATE.dt.year < 2019].copy(deep=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine our data at the city level for compatibility with our other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_13_19['CITYSTATE'] = reac_13_19.CITY.str.upper() + ',' + reac_13_19.STATE.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_13_19 = reac_13_19 \\\n",
    "    .groupby(by='CITYSTATE') \\\n",
    "    .INSPECTION_SCORE \\\n",
    "    .mean() \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'INSPECTION_SCORE': 'AVG_SCORE'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY_OUT = '../data/processed/'\n",
    "\n",
    "reac_13_19.to_csv(\n",
    "    WORKING_DIRECTORY_OUT + 'reac_13-19.csv', \n",
    "    sep=',', \n",
    "    index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
