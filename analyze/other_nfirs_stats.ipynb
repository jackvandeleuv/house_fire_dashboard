{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIRECTORY = '../data/processed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.Connection(WORKING_DIRECTORY + 'fire_data.db')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic cleaning and validation\n",
    "\n",
    "Author: Jack Vandeleuv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll get basic summary stats about fires for U.S. cities based on the NFIRS data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with these columns:\n",
    "* EXP_NO (exposure number): the number of other structures/vehicles to which the fire spreads. By calculating the average exposure number by city, we can see which cities are most likely to see fires spread.\n",
    "* PROP_LOSS and CONT_LOSS: the dollar values of lost property and lost contents (anything inside a burning structure/vehicle) respectively. There is also PROP_VAL and CONT_VAL, but these measure the pre-fire value and so we'll ignore them.\n",
    "* OTH_DEATH, OTH_INJ, FF_DEATH, OTH_INJ: measures deaths and injuries for non-firefighters and firefighters respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we query values from the table, let's look at percentage of null values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 15 seconds to run\n",
    "# pd.options.display.float_format = '{:.6f}'.format\n",
    "# pd.read_sql(\"\"\"\n",
    "#     SELECT\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(INCIDENT_KEY)) / COUNT(*) AS INCIDENT_KEY_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(INC_DATE)) / COUNT(*) AS INC_DATE_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(EXP_NO)) / COUNT(*) AS EXP_NO_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(ALARMS)) / COUNT(*) AS ALARMS_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(PROP_LOSS)) / COUNT(*) AS PROP_LOSS_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(CONT_LOSS)) / COUNT(*) AS CONT_LOSS_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(OTH_DEATH)) / COUNT(*) AS OTH_DEATH_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(FF_DEATH)) / COUNT(*) AS FF_DEATH_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(OTH_INJ)) / COUNT(*) AS OTH_INJ_NULL,\n",
    "#         (CAST(COUNT(*) AS FLOAT) - COUNT(FF_INJ)) / COUNT(*) AS FF_INJ_NULL\n",
    "#     FROM basic_incident;\n",
    "# \"\"\", conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have significant numbers of null values for:\n",
    "* ALARMS\n",
    "* PROPERTY LOSS AND CONTENTS LOSS\n",
    "* INJURY AND DEATH (FIREFIGHTER AND NON-FIREFIGHTER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a basic sanity check: find the total number of deaths there would be, assuming that the average can be calculated by disregarding the null values. (In reality, about 3,000 Americans die in fires every year.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 45 seconds to run\n",
    "# pd.options.display.float_format = '{:.1f}'.format\n",
    "# pd.read_sql(\"\"\"\n",
    "#     SELECT\n",
    "#         CAST(SUBSTR(INC_DATE, LENGTH(INC_DATE) - 3, 4) AS INTEGER) AS year,\n",
    "#         AVG(FF_DEATH) * COUNT(*) AS ff_death_no_imputation,\n",
    "#         SUM(IFNULL(FF_DEATH, 0)) AS ff_death_impute_zero,\n",
    "#         AVG(OTH_DEATH) * COUNT(*) AS other_death_no_imputation,\n",
    "#         SUM(IFNULL(OTH_DEATH, 0)) AS other_death_impute_zero\n",
    "#     FROM basic_incident\n",
    "#     GROUP BY CAST(SUBSTR(INC_DATE, LENGTH(INC_DATE) - 3, 4) AS INTEGER)\n",
    "# ;\"\"\", conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~500K/year is much too high for fire deaths! So imputation is likely the correct approach for fatalities. There is less effect on firefighter deaths, as these values are mostly not null.\n",
    "\n",
    "We'll do a similar validation with property (and contents of buildings) lost to fire, expressed in dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.5 minutes to run\n",
    "# pd.options.display.float_format = '${:,.2f}'.format\n",
    "# loss = pd.read_sql(\"\"\"\n",
    "#     SELECT\n",
    "#         CAST(SUBSTR(INC_DATE, LENGTH(INC_DATE) - 3, 4) AS INTEGER) AS year,\n",
    "#         AVG(PROP_LOSS) * COUNT(*) AS property_loss_no_imputation,\n",
    "#         SUM(IFNULL(PROP_LOSS, 0)) AS property_loss_impute_zero,\n",
    "#         AVG(CONT_LOSS) * COUNT(*) AS contents_loss_no_imputation,\n",
    "#         SUM(IFNULL(CONT_LOSS, 0)) AS contents_loss_impute_zero\n",
    "#     FROM basic_incident\n",
    "#     GROUP BY CAST(SUBSTR(INC_DATE, LENGTH(INC_DATE) - 3, 4) AS INTEGER)\n",
    "# ;\"\"\", conn)\n",
    "\n",
    "# display(loss)\n",
    "# pd.reset_option('display.float_format')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This validation is less clear-cut, with the property loss being about twice as high without imputation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on, we will impute these as 0 to avoid inflating the average values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate average death, injury, property loss, and fire spread by city"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll impute 0 when death/injury, property/contents loss, or alarms is null."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll calcuate the average exposure number per city. Each fire (including exposure fires resulting from the first fire), shares a STATE/FDID/INC_DATE/INC_NO. (This is the INCIDENT_KEY minus the EXP_NO, or exposure number, which is the final part of the 5-part key.)\n",
    "\n",
    "By taking the maximum exposure number in the subquery, we'll get a count of the number of exposures per fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 minutes\n",
    "avg_exp = pd.read_sql(\"\"\"\n",
    "    WITH sub AS (\n",
    "        SELECT \n",
    "            ia.CITY,\n",
    "            ia.STATE, \n",
    "            ia.FDID, \n",
    "            ia.INC_DATE, \n",
    "            ia.INC_NO,\n",
    "            MAX(ia.EXP_NO) as max\n",
    "        FROM incident_address ia\n",
    "            JOIN basic_incident bi\n",
    "            USING (INCIDENT_KEY)\n",
    "        WHERE bi.INC_TYPE < 200 \n",
    "        GROUP BY ia.CITY, ia.STATE, ia.FDID, ia.INC_DATE, ia.INC_NO\n",
    "    )\n",
    "    SELECT \n",
    "        sub.CITY || ',' || sub.STATE AS CITYSTATE,\n",
    "        SUM(sub.max) AS SUM_SPREAD,\n",
    "        COUNT(*) AS SPREAD_SUPPORT,\n",
    "        CAST(SUBSTR(sub.INC_DATE, LENGTH(sub.INC_DATE) - 3, 4) AS INTEGER) as YEAR \n",
    "    FROM sub\n",
    "    GROUP BY \n",
    "        sub.CITY, \n",
    "        sub.STATE, \n",
    "        CAST(SUBSTR(sub.INC_DATE, LENGTH(sub.INC_DATE) - 3, 4) AS INTEGER)\n",
    "\"\"\", conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to consider that factors like property loss may be reported separately for different sub-fires. E.g., if an initial fire causes two other fires, each of the three fires might have its own associated property loss, which will be reported under a separate INCIDENT_KEY. \n",
    "\n",
    "We can aggregate the death, injury, and property/container loss by grouping by STATE, FDID, INC_DATE, and INC_NO. \n",
    "\n",
    "(INCIDENT_KEY, which uniquely identifies a fire, is a composite string formatted like this => STATE + FDID + INC_DATE + INC_NO + EXP_NO.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting all rows in incident address where exposure number is zero will give us all reported primary fires (i.e. fires not as the result of an exposure from another fire) where the address is known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 minutes to run\n",
    "avg_loss = pd.read_sql(\"\"\"\n",
    "    SELECT\n",
    "        ia.CITY || ',' || ia.STATE AS CITYSTATE,\n",
    "        SUM(COALESCE(bi.PROP_LOSS, 0)) as SUM_PROP_LOSS,\n",
    "        SUM(COALESCE(bi.CONT_LOSS, 0)) as SUM_CONT_LOSS,\n",
    "        SUM(COALESCE(bi.OTH_DEATH, 0)) as SUM_OTH_DEATH,\n",
    "        SUM(COALESCE(bi.FF_DEATH, 0)) as SUM_FF_DEATH,\n",
    "        SUM(COALESCE(bi.OTH_INJ, 0)) as SUM_OTH_INJ,\n",
    "        SUM(COALESCE(bi.FF_INJ, 0)) as SUM_FF_INJ,\n",
    "        COUNT(*) AS LOSS_SUPPORT,\n",
    "        CAST(SUBSTR(ia.INC_DATE, LENGTH(ia.INC_DATE) - 3, 4) AS INTEGER) as YEAR\n",
    "    FROM basic_incident bi \n",
    "        JOIN incident_address ia\n",
    "        USING (INCIDENT_KEY)\n",
    "    WHERE bi.INC_TYPE < 200 \n",
    "    GROUP BY \n",
    "        ia.CITY, \n",
    "        ia.STATE, \n",
    "        CAST(SUBSTR(ia.INC_DATE, LENGTH(ia.INC_DATE) - 3, 4) AS INTEGER)\n",
    "\"\"\", conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll export the results for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss.to_csv(\n",
    "    WORKING_DIRECTORY + 'nfirs_loss.csv',\n",
    "    sep=',',\n",
    "    index=False\n",
    ")\n",
    "\n",
    "avg_exp.to_csv(\n",
    "    WORKING_DIRECTORY + 'nfirs_spread.csv',\n",
    "    sep=',',\n",
    "    index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
