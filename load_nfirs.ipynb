{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import Session\n",
    "import os\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading NFIRS incidentaddress.txt into SQLite db file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEMA provides the NFIRS datasets as multiple CSV files covering multiple years. There are also two different table schemas for each year.\n",
    "\n",
    "We'll load this data into a local SQLite db file, which will enable us to more easily geocode our addresses, allowing us to compare between the NFIRS and HUD REAC datasets.\n",
    "\n",
    "For our purposes, we'll only load the \"incidentaddress.txt\" and \"basicincident.txt\" files into SQL. These files contain data on the location of the incidents and the nature of the incidents respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = 'D:/Fire Project/data/'\n",
    "NFIRS_PATHS = ['nfirs_fire_hazmat_pdr_2020/NFIRS_FIRES_2020_022322',\n",
    "                 'USFA NFIRS 2019 Hazmat/NFIRS_FIRES_2019_011921',\n",
    "                 'USFA NFIRS 2018 Hazmat/NFIRS_FIRES_2018_110119',\n",
    "                 'USFA NFIRS 2017 Hazmat/NFIRS_FIRES_2017_020719',\n",
    "                 'USFA NFIRS 2016 Hazmat/NFIRS_FIRES_2016_02-05-2018',\n",
    "                 'USFA NFIRS 2015 Hazmat/NFIRS_FIRES_2015_20170215']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a local sqlite3 database file so that we can easily store our data as we add geocodes to the existing addresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table for incidentaddresses.\n",
    "conn = sqlite3.Connection('fire_data_copy.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS incident_address (\n",
    "    INTEGER PRIMARY KEY,\n",
    "    INCIDENT_KEY TEXT,\n",
    "    STATE TEXT,\n",
    "    FDID INTEGER,\n",
    "    INC_DATE INTEGER,\n",
    "    INC_NO INTEGER,\n",
    "    EXP_NO INTEGER,\n",
    "    LOC_TYPE INTEGER,\n",
    "    NUM_MILE INTEGER,\n",
    "    STREET_PRE TEXT,\n",
    "    STREETNAME TEXT,\n",
    "    STREETTYPE TEXT,\n",
    "    STREETSUF TEXT,\n",
    "    APT_NO TEXT,\n",
    "    CITY TEXT,\n",
    "    STATE_ID TEXT,\n",
    "    ZIP5 INTEGER,\n",
    "    ZIP4 INTEGER,\n",
    "    X_STREET TEXT\n",
    ")\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS basic_incident (\n",
    "        ID INTEGER PRIMARY KEY,\n",
    "        STATE TEXT,\n",
    "        FDID INTEGER,\n",
    "        INC_DATE TEXT,\n",
    "        INC_NO INTEGER,\n",
    "        EXP_NO INTEGER,\n",
    "        VERSION REAL,\n",
    "        DEPT_STA TEXT,\n",
    "        INC_TYPE INTEGER,\n",
    "        ADD_WILD INTEGER,\n",
    "        AID TEXT,\n",
    "        ALARM INTEGER,\n",
    "        ARRIVAL TEXT,\n",
    "        INC_CONT TEXT,\n",
    "        LU_CLEAR REAL,\n",
    "        SHIFT TEXT,\n",
    "        ALARMS INTEGER,\n",
    "        DISTRICT INTEGER,\n",
    "        ACT_TAK1 REAL,\n",
    "        ACT_TAK2 INTEGER,\n",
    "        ACT_TAK3 INTEGER,\n",
    "        APP_MOD TEXT,\n",
    "        SUP_APP INTEGER,\n",
    "        EMS_APP INTEGER,\n",
    "        OTH_APP INTEGER,\n",
    "        SUP_PER INTEGER,\n",
    "        EMS_PER INTEGER,\n",
    "        OTH_PER INTEGER,\n",
    "        RESOU_AID TEXT,\n",
    "        PROP_LOSS REAL,\n",
    "        CONT_LOSS REAL,\n",
    "        PROP_VAL REAL,\n",
    "        CONT_VAL REAL,\n",
    "        FF_DEATH REAL,\n",
    "        OTH_DEATH REAL,\n",
    "        FF_INJ REAL,\n",
    "        OTH_INJ REAL,\n",
    "        DET_ALERT TEXT,\n",
    "        HAZ_REL TEXT,\n",
    "        MIXED_USE TEXT,\n",
    "        PROP_USE INTEGER,\n",
    "        CENSUS INTEGER,\n",
    "        INCIDENT_KEY TEXT,\n",
    "        FOREIGN KEY (INCIDENT_KEY) \n",
    "            REFERENCES INCIDENT_ADDRESS (INCIDENT_KEY)\n",
    "    )\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll load our csv files, each called incidentaddress.txt, and put them in the same SQL table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets from 2015-2018 doesn't have an INCIDENT_KEY column, so we will construct one out of the other information in the dataset. This format, with five components, is consistent with the INCIDENT_KEY field in 2019-2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.Connection('fire_data_copy.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Append each dataframe to existing table.\n",
    "# for path in NFIRS_PATHS:\n",
    "#     df = pd.read_csv(WORKING_DIR + path + '/incidentaddress.txt', \n",
    "#                         sep='^',\n",
    "#                         low_memory=False,\n",
    "#                         # Specify alternative text encoding.\n",
    "#                         encoding='ISO-8859-1')\n",
    "    \n",
    "    # # The CSVs from 2018 and earlier have 17 columns (instead of 18)\n",
    "#     if len(df.columns) == 17:\n",
    "#         incident_key = df.loc[:, ['STATE', 'FDID', 'INC_DATE', 'INC_NO', 'EXP_NO']].astype(str)\n",
    "#         df['INCIDENT_KEY'] = incident_key.agg('_'.join, axis=1)\n",
    "    \n",
    "#     df.to_sql('incident_address',\n",
    "#                     conn, \n",
    "#                     if_exists='append', \n",
    "#                     index=False)\n",
    "#     conn.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll load the data from the 2015-2020 basicincident.txt files, which will have a foreign key \"INCIDENT_KEY\" that will connect to the column with the same name in incident_address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conn = sqlite3.Connection('fire_data_copy.db')\n",
    "# cur = conn.cursor()\n",
    "\n",
    "# # Append each dataframe to existing table.\n",
    "# for path in NFIRS_PATHS:\n",
    "#     df = pd.read_csv(WORKING_DIR + path + '/basicincident.txt', \n",
    "#                         sep='^',\n",
    "#                         low_memory=False,\n",
    "#                         # Specify alternative text encoding.\n",
    "#                         encoding='ISO-8859-1')\n",
    "    \n",
    "#     # The CSVs from 2018 and earlier have 41 columns (instead of 42)\n",
    "#     if len(df.columns) == 41:\n",
    "#         incident_key = df.loc[:, ['STATE', 'FDID', 'INC_DATE', 'INC_NO', 'EXP_NO']].astype(str)\n",
    "#         df['INCIDENT_KEY'] = incident_key.agg('_'.join, axis=1)\n",
    "    \n",
    "#     df.to_sql('basic_incident',\n",
    "#                     conn, \n",
    "#                     if_exists='append', \n",
    "#                     index=False)\n",
    "#     conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
