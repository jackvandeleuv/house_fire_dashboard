{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize placenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_fixed = pd.read_csv('../data/processed/reac_fixed.csv')\n",
    "nfirs_fixed = pd.read_csv('../data/processed/nfirs_fixed.csv')\n",
    "nspire_fixed = pd.read_csv('../data/processed/nspire_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_dict = {}\n",
    "\n",
    "reac_fixes = reac_fixed[reac_fixed.citystate != reac_fixed.corrected_address] \\\n",
    "    .loc[:, ['corrected_address', 'citystate']] \\\n",
    "    .values.tolist()\n",
    "for fix in reac_fixes:\n",
    "    fix_dict[fix[1]] = fix[0]\n",
    "\n",
    "nfirs_fixes = nfirs_fixed[nfirs_fixed.citystate != nfirs_fixed.corrected_address] \\\n",
    "    .loc[:, ['corrected_address', 'citystate']] \\\n",
    "    .values.tolist()\n",
    "for fix in nfirs_fixes:\n",
    "    fix_dict[fix[1]] = fix[0]\n",
    "\n",
    "nspire_fixes = nspire_fixed[nspire_fixed.citystate != nspire_fixed.corrected_address] \\\n",
    "    .loc[:, ['corrected_address', 'citystate']] \\\n",
    "    .values.tolist()\n",
    "for fix in nspire_fixes:\n",
    "    fix_dict[fix[1]] = fix[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize REAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_13_18 = pd.read_csv('../data/processed/reac_13-18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_13_18['CITYSTATE'] = reac_13_18.CITYSTATE.apply(\n",
    "    lambda x: fix_dict[x]\n",
    "    if x in fix_dict\n",
    "    else x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average latitude and longitude for each CITYSTATE\n",
    "avg_coordinates = reac_13_18 \\\n",
    "    .loc[:, ['CITYSTATE', 'LATITUDE', 'LONGITUDE']] \\\n",
    "    .groupby('CITYSTATE') \\\n",
    "    .mean() \\\n",
    "    .reset_index()\n",
    "\n",
    "# Calculate the average inspection scores for each INSPECTION_TYPE\n",
    "avg_scores = reac_13_18 \\\n",
    "    .loc[:, ['CITYSTATE', 'INSPECTION_SCORE', 'INSPECTION_TYPE']] \\\n",
    "    .groupby(['CITYSTATE', 'INSPECTION_TYPE']) \\\n",
    "    .mean() \\\n",
    "    .reset_index() \\\n",
    "    .pivot(index='CITYSTATE', columns='INSPECTION_TYPE', values='INSPECTION_SCORE') \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={\n",
    "        'MULTIFAMILY': 'AVG_SCORE_MULTIFAMILY',\n",
    "        'PUBLIC': 'AVG_SCORE_PUBLIC'\n",
    "    }) \\\n",
    "    .rename_axis(None, axis=1)\n",
    "\n",
    "# Join the two DataFrames together on the CITYSTATE column\n",
    "reac_13_18 = avg_coordinates.merge(avg_scores, on='CITYSTATE')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize NFIRS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For NFIRS data, the data is aggregated at the level of city/locality/town/etc, so if we standardize a name we need to combine metrics relying on an average with a weighted average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfirs_loss = pd.read_csv('../data/processed/nfirs_loss.csv')\n",
    "nfirs_spread = pd.read_csv('../data/processed/nfirs_spread.csv')\n",
    "nfirs_top5 = pd.read_csv('../data/processed/top5_by_type.csv')\n",
    "nfirs_counts = pd.read_csv('../data/processed/total_incident_count.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows without a locality name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfirs_loss = nfirs_loss[nfirs_loss.CITYSTATE.notna()]\n",
    "nfirs_spread = nfirs_spread[nfirs_spread.CITYSTATE.notna()]\n",
    "nfirs_top5 = nfirs_top5[nfirs_top5.CITYSTATE.notna()]\n",
    "nfirs_counts = nfirs_counts[nfirs_counts.CITYSTATE.notna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove empty spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_space(x):\n",
    "    return str(x).strip().replace(', ', ',').replace(' ,', ',')\n",
    "\n",
    "nfirs_loss['CITYSTATE'] = nfirs_loss.CITYSTATE.apply(remove_empty_space)\n",
    "nfirs_spread['CITYSTATE'] = nfirs_spread.CITYSTATE.apply(remove_empty_space)\n",
    "nfirs_top5['CITYSTATE'] = nfirs_top5.CITYSTATE.apply(remove_empty_space)\n",
    "nfirs_counts['CITYSTATE'] = nfirs_counts.CITYSTATE.apply(remove_empty_space)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize non-standard names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_names(x):\n",
    "    if x in fix_dict:\n",
    "        return fix_dict[x]\n",
    "    return x\n",
    "\n",
    "nfirs_loss['CITYSTATE'] = nfirs_loss.CITYSTATE.apply(standardize_names)\n",
    "nfirs_spread['CITYSTATE'] = nfirs_spread.CITYSTATE.apply(standardize_names)\n",
    "nfirs_top5['CITYSTATE'] = nfirs_top5.CITYSTATE.apply(standardize_names)\n",
    "nfirs_counts['CITYSTATE'] = nfirs_counts.CITYSTATE.apply(standardize_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the summed values in the DataFrames by CITYSTATE and YEAR (standardizing names may have introduced duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfirs_loss = nfirs_loss.groupby(by=['CITYSTATE', 'YEAR']).sum().reset_index()\n",
    "nfirs_spread = nfirs_spread.groupby(by=['CITYSTATE', 'YEAR']).sum().reset_index()\n",
    "nfirs_counts = nfirs_counts.groupby(by=['CITYSTATE', 'YEAR']).sum().reset_index()\n",
    "nfirs_top5 = nfirs_top5.groupby(by=['CITYSTATE', 'YEAR']).sum().reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize Census Populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = pd.read_csv('../data/processed/populations_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sumlev = pops.groupby('CITYSTATE')['SUMLEV'].transform(max)\n",
    "pops = pops[pops['SUMLEV'] == max_sumlev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = pops.drop_duplicates(subset=['CITYSTATE'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = pops.loc[:, ['CITYSTATE', 'POPULATION']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize NSPIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "nspire = pd.read_csv('../data/processed/nspire_partial_clean.csv',\n",
    "                     low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nspire['CITYSTATE'] = nspire['Shipping City'].str.upper() \\\n",
    "                    + ',' \\\n",
    "                    + nspire['Shipping State/Province'].str.upper()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no CITYSTATE value, remove empty spaces around locality & state names, and standardize locality names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "nspire = nspire[nspire.CITYSTATE.notna()]\n",
    "nspire['CITYSTATE'] = nspire.copy().CITYSTATE.apply(remove_empty_space)\n",
    "nspire['CITYSTATE'] = nspire.copy().CITYSTATE.apply(standardize_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get number of inspections per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspections_per_city = nspire \\\n",
    "    .groupby(by=['Shipping City', 'Inspection ID']) \\\n",
    "    .size() \\\n",
    "    .reset_index()\n",
    "per_city = inspections_per_city.groupby(by='Shipping City')['Inspection ID'].count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, there are only 2.33 NSPIRE inspections per city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1275.000000\n",
       "mean        2.334118\n",
       "std         4.624661\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max        95.000000\n",
       "Name: Inspection ID, dtype: float64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_city.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_standards = [\n",
    "    \"Carbon Monoxide Alarm\",\n",
    "    \"Leak\",\n",
    "    \"Chimney\",\n",
    "    \"Door - Fire Labeled\",\n",
    "    \"Electrical - Outlet and Switch\",\n",
    "    \"Electrical - Conductor\",\n",
    "    \"Electrical - Service Panel\",\n",
    "    \"Electrical - GFCI or AFCI Outlet or Breaker\",\n",
    "    \"Fire Escape\",\n",
    "    \"Fire Extinguisher\",\n",
    "    \"Flammable and Combustible Item\",\n",
    "    \"Smoke Alarm\",\n",
    "    \"Heating, Ventilation, and Air Conditioning (HVAC)\",\n",
    "    \"Kitchen Ventilation\",\n",
    "    \"Structural System\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for standard in fire_standards:\n",
    "    nspire = pd.merge(\n",
    "        nspire, \n",
    "        nspire[nspire['NSPIRE Standards'] == standard] \\\n",
    "            .groupby(by='CITYSTATE') \\\n",
    "            .size() \\\n",
    "            .reset_index(),\n",
    "        on='CITYSTATE',\n",
    "        how='left'\n",
    "    )\n",
    "    nspire = nspire.rename(columns={nspire.columns[-1]: 'Count ' + standard})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col_names = ['Count ' + s for s in fire_standards]\n",
    "nspire_filter = nspire.loc[:, ['CITYSTATE'] + new_col_names]\n",
    "nspire = nspire_filter \\\n",
    "    .groupby(by='CITYSTATE') \\\n",
    "    .mean() \\\n",
    "    .reset_index() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our NFIRS data has year values, so we'll need to combine CITYSTATE and YEAR to get a column to merge on. For population, we'll create a column using duplicate values for each year (e.g. NEW YORK,NY,2013 and NEW YORK,NY,2020 will have the same population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfirs_loss['CITYSTATE_YEAR'] = nfirs_loss.CITYSTATE.str.upper() + ',' + nfirs_loss.YEAR.astype(str)\n",
    "nfirs_spread['CITYSTATE_YEAR'] = nfirs_spread.CITYSTATE.str.upper() + ',' + nfirs_spread.YEAR.astype(str)\n",
    "nfirs_counts['CITYSTATE_YEAR'] = nfirs_counts.CITYSTATE.str.upper() + ',' + nfirs_counts.YEAR.astype(str)\n",
    "nfirs_top5['CITYSTATE_YEAR'] = nfirs_top5.CITYSTATE.str.upper() + ',' + nfirs_top5.YEAR.astype(str)\n",
    "pop_df = pops.copy(deep=True)\n",
    "for year in nfirs_loss.YEAR.unique():\n",
    "    pop_df_sub = pops.copy(deep=True)\n",
    "    pop_df_sub['CITYSTATE_YEAR'] = pop_df_sub.CITYSTATE.str.upper() + ',' + str(year)\n",
    "    pop_df = pd.concat([pop_df, pop_df_sub], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "reac_cities = reac_13_18.CITYSTATE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jackv\\AppData\\Local\\Temp\\ipykernel_16380\\235890827.py:9: FutureWarning: Passing 'suffixes' which cause duplicate columns {'CITYSTATE_x', 'YEAR_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  merged = pd.merge(left=merged, right=nfirs_top5, on='CITYSTATE_YEAR', how='inner')\n"
     ]
    }
   ],
   "source": [
    "# merged = pd.merge(left=nspire, right=nfirs_loss, on='CITYSTATE', how='outer')\n",
    "# merged = pd.merge(left=merged, right=nfirs_spread, on='CITYSTATE', how='outer')\n",
    "# merged = pd.merge(left=merged, right=nfirs_counts, on='CITYSTATE', how='outer')\n",
    "# merged = pd.merge(left=merged, right=nfirs_top5, on='CITYSTATE', how='outer')\n",
    "# merged = pd.merge(left=merged, right=pops, on='CITYSTATE', how='left')\n",
    "\n",
    "merged = pd.merge(left=nfirs_loss, right=nfirs_spread, on='CITYSTATE_YEAR', how='inner')\n",
    "merged = pd.merge(left=merged, right=nfirs_counts, on='CITYSTATE_YEAR', how='inner')\n",
    "merged = pd.merge(left=merged, right=nfirs_top5, on='CITYSTATE_YEAR', how='inner')\n",
    "merged = pd.merge(left=merged, right=pop_df, on='CITYSTATE_YEAR', how='inner')\n",
    "\n",
    "# Filter by cities that have some public housing\n",
    "merged = merged[merged.CITYSTATE.isin(reac_cities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv('../data/processed/nfirs_pop_merge_filter_reac.csv', index=False, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
